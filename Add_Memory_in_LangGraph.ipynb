{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Short Term Memory"
      ],
      "metadata": {
        "id": "5z24vt1MfMr9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqfyqjyDfEYP",
        "outputId": "ece2de24-4b39-4d85-b36c-ac0cf8da0b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.6.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from langchain_groq) (0.31.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.14.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community langchain_groq langgraph"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sINJKQ_3fFfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing required libraries\n",
        "from langchain_groq import ChatGroq\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from typing import TypedDict, List, Annotated\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langgraph.graph.message import AnyMessage, add_messages\n",
        "import os"
      ],
      "metadata": {
        "id": "JZHntrOvfFh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading API key\n",
        "from google.colab import userdata\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "KynZhrU8fFlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM\n",
        "llm = ChatGroq(model = \"llama-3.3-70b-versatile\", api_key = groq_api_key)"
      ],
      "metadata": {
        "id": "M6ioTAHkfFqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the State\n",
        "class State(TypedDict):\n",
        "  messages: Annotated[List[AnyMessage], add_messages]"
      ],
      "metadata": {
        "id": "5dxU9V6mfFtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt\n",
        "prompts = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"{system_messages}\"),\n",
        "        MessagesPlaceholder(\"messages\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "llm_model = prompts | llm\n",
        "\n",
        "#Build the Graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "def chatnode(state:State):\n",
        "  system_messages = \"You are an assitant\"\n",
        "  state[\"messages\"] = llm_model.invoke({\"system_messages\": system_messages, \"messages\": state[\"messages\"]})\n",
        "  return state\n",
        "\n",
        "#Add node\n",
        "workflow.add_node(\"chatnode\", chatnode)\n",
        "#Add edges\n",
        "workflow.add_edge(START, \"chatnode\")\n",
        "workflow.add_edge(\"chatnode\", END)\n",
        "\n",
        "#Compile the graph\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "KkbRTmlAgZa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First question\n",
        "response = graph.invoke({\"messages\": \"Heyy, My name is Krish\"})\n",
        "for message in response[\"messages\"]:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzzbZ-yIgZeh",
        "outputId": "88b32586-2f7f-4664-dd34-bcb8bbfc8fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, My name is Krish\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Krish! It's nice to meet you. Is there something I can help you with or would you like to chat? I'm all ears!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Second question\n",
        "response2 = graph.invoke({\"messages\": \"Heyy, Do you remember my name?\"})\n",
        "for message in response2[\"messages\"]:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y4_zzXQgZis",
        "outputId": "36f72916-d4e7-45e1-f494-4a3d5faa3a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, Do you remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm happy to chat with you. However, I'm a large language model, I don't have personal memories, so I don't recall previous conversations or names. Each time you interact with me, it's a new conversation. If you'd like to share your name, I'd be happy to use it in our conversation though!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3NZJEw4lO_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "#Build the Graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "def chatnode(state:State):\n",
        "  system_messages = \"You are an assitant\"\n",
        "  state[\"messages\"] = llm_model.invoke({\"system_messages\": system_messages, \"messages\": state[\"messages\"]})\n",
        "  return state\n",
        "\n",
        "#Add node\n",
        "workflow.add_node(\"chatnode\", chatnode)\n",
        "#Add edges\n",
        "workflow.add_edge(START, \"chatnode\")\n",
        "workflow.add_edge(\"chatnode\", END)\n",
        "\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "#Compile the graph\n",
        "graph = workflow.compile(checkpointer = checkpointer)"
      ],
      "metadata": {
        "id": "lbCn-Y3tgZmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "#First question\n",
        "response = graph.invoke({\"messages\": \"Heyy, My name is Krish\"}, config = config1)\n",
        "for message in response[\"messages\"]:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1eO3pStgZpx",
        "outputId": "755ab747-d445-4604-eb37-e193b832ccd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, My name is Krish\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Second question\n",
        "response2 = graph.invoke({\"messages\": \"Heyy, Do you remember my name?\"}, config = config1)\n",
        "for message in response2[\"messages\"]:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1Un7PF6gZtj",
        "outputId": "3eca897b-f829-4c3d-c096-3a5dafe65840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, My name is Krish\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, Do you remember my name?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Krish, right? I remember! We just met a minute ago. How can I assist you today, Krish?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.get_state(config= config1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgEC4qJDgZxW",
        "outputId": "64c6b222-e378-49e1-b11e-bfc4e20c5d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StateSnapshot(values={'messages': [HumanMessage(content='Heyy, My name is Krish', additional_kwargs={}, response_metadata={}, id='ec38309f-ac25-45bb-814d-0ab77356bd9c'), AIMessage(content=\"Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 47, 'total_tokens': 78, 'completion_time': 0.065060804, 'prompt_time': 0.016321655, 'queue_time': 0.220762273, 'total_time': 0.081382459}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b51da721-38d5-4e72-9b50-b36a933b6d70-0', usage_metadata={'input_tokens': 47, 'output_tokens': 31, 'total_tokens': 78}), HumanMessage(content='Heyy, Do you remember my name?', additional_kwargs={}, response_metadata={}, id='fa102a97-7dc4-48a3-9491-e4c4e01db61b'), AIMessage(content='Your name is Krish, right? I remember! We just met a minute ago. How can I assist you today, Krish?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 96, 'total_tokens': 123, 'completion_time': 0.095182298, 'prompt_time': 0.014939306, 'queue_time': 0.217410122, 'total_time': 0.110121604}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--de4c1c38-5ed5-408d-b246-73e9c70d7a4d-0', usage_metadata={'input_tokens': 96, 'output_tokens': 27, 'total_tokens': 123})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-3041-6f43-8004-bb0780c8af4b'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-08-18T05:10:46.921166+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-2c45-64cc-8003-89b27ad771d4'}}, tasks=(), interrupts=())"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(graph.get_state_history(config1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5RV_pGzgZ0o",
        "outputId": "0ce60e9d-bbde-4388-ea46-9b4ee88019db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[StateSnapshot(values={'messages': [HumanMessage(content='Heyy, My name is Krish', additional_kwargs={}, response_metadata={}, id='ec38309f-ac25-45bb-814d-0ab77356bd9c'), AIMessage(content=\"Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 47, 'total_tokens': 78, 'completion_time': 0.065060804, 'prompt_time': 0.016321655, 'queue_time': 0.220762273, 'total_time': 0.081382459}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b51da721-38d5-4e72-9b50-b36a933b6d70-0', usage_metadata={'input_tokens': 47, 'output_tokens': 31, 'total_tokens': 78}), HumanMessage(content='Heyy, Do you remember my name?', additional_kwargs={}, response_metadata={}, id='fa102a97-7dc4-48a3-9491-e4c4e01db61b'), AIMessage(content='Your name is Krish, right? I remember! We just met a minute ago. How can I assist you today, Krish?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 96, 'total_tokens': 123, 'completion_time': 0.095182298, 'prompt_time': 0.014939306, 'queue_time': 0.217410122, 'total_time': 0.110121604}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--de4c1c38-5ed5-408d-b246-73e9c70d7a4d-0', usage_metadata={'input_tokens': 96, 'output_tokens': 27, 'total_tokens': 123})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-3041-6f43-8004-bb0780c8af4b'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-08-18T05:10:46.921166+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-2c45-64cc-8003-89b27ad771d4'}}, tasks=(), interrupts=()),\n",
              " StateSnapshot(values={'messages': [HumanMessage(content='Heyy, My name is Krish', additional_kwargs={}, response_metadata={}, id='ec38309f-ac25-45bb-814d-0ab77356bd9c'), AIMessage(content=\"Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 47, 'total_tokens': 78, 'completion_time': 0.065060804, 'prompt_time': 0.016321655, 'queue_time': 0.220762273, 'total_time': 0.081382459}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b51da721-38d5-4e72-9b50-b36a933b6d70-0', usage_metadata={'input_tokens': 47, 'output_tokens': 31, 'total_tokens': 78}), HumanMessage(content='Heyy, Do you remember my name?', additional_kwargs={}, response_metadata={}, id='fa102a97-7dc4-48a3-9491-e4c4e01db61b')]}, next=('chatnode',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-2c45-64cc-8003-89b27ad771d4'}}, metadata={'source': 'loop', 'step': 3, 'parents': {}}, created_at='2025-08-18T05:10:46.503120+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-2c42-6b0a-8002-21017df27325'}}, tasks=(PregelTask(id='cde372c8-d27e-a1ae-f82f-4d61ea12850c', name='chatnode', path=('__pregel_pull', 'chatnode'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Krish, right? I remember! We just met a minute ago. How can I assist you today, Krish?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 96, 'total_tokens': 123, 'completion_time': 0.095182298, 'prompt_time': 0.014939306, 'queue_time': 0.217410122, 'total_time': 0.110121604}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--de4c1c38-5ed5-408d-b246-73e9c70d7a4d-0', usage_metadata={'input_tokens': 96, 'output_tokens': 27, 'total_tokens': 123})}),), interrupts=()),\n",
              " StateSnapshot(values={'messages': [HumanMessage(content='Heyy, My name is Krish', additional_kwargs={}, response_metadata={}, id='ec38309f-ac25-45bb-814d-0ab77356bd9c'), AIMessage(content=\"Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 47, 'total_tokens': 78, 'completion_time': 0.065060804, 'prompt_time': 0.016321655, 'queue_time': 0.220762273, 'total_time': 0.081382459}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b51da721-38d5-4e72-9b50-b36a933b6d70-0', usage_metadata={'input_tokens': 47, 'output_tokens': 31, 'total_tokens': 78})]}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf1b-2c42-6b0a-8002-21017df27325'}}, metadata={'source': 'input', 'step': 2, 'parents': {}}, created_at='2025-08-18T05:10:46.502043+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf19-94fd-6479-8001-736aff9c911c'}}, tasks=(PregelTask(id='0af234f5-cc93-3a9c-7a7b-9c77fe39554c', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': 'Heyy, Do you remember my name?'}),), interrupts=()),\n",
              " StateSnapshot(values={'messages': [HumanMessage(content='Heyy, My name is Krish', additional_kwargs={}, response_metadata={}, id='ec38309f-ac25-45bb-814d-0ab77356bd9c'), AIMessage(content=\"Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 47, 'total_tokens': 78, 'completion_time': 0.065060804, 'prompt_time': 0.016321655, 'queue_time': 0.220762273, 'total_time': 0.081382459}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b51da721-38d5-4e72-9b50-b36a933b6d70-0', usage_metadata={'input_tokens': 47, 'output_tokens': 31, 'total_tokens': 78})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf19-94fd-6479-8001-736aff9c911c'}}, metadata={'source': 'loop', 'step': 1, 'parents': {}}, created_at='2025-08-18T05:10:03.796560+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf19-9109-67be-8000-f89be35aab25'}}, tasks=(), interrupts=()),\n",
              " StateSnapshot(values={'messages': [HumanMessage(content='Heyy, My name is Krish', additional_kwargs={}, response_metadata={}, id='ec38309f-ac25-45bb-814d-0ab77356bd9c')]}, next=('chatnode',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf19-9109-67be-8000-f89be35aab25'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-08-18T05:10:03.382128+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf19-90e0-68ad-bfff-3f36c8e9eb6b'}}, tasks=(PregelTask(id='2d5b4780-0f18-13ad-dda2-9c4225dc941e', name='chatnode', path=('__pregel_pull', 'chatnode'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content=\"Hello Krish! It's nice to meet you! Is there something I can help you with or would you like to chat? I'm all ears!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 47, 'total_tokens': 78, 'completion_time': 0.065060804, 'prompt_time': 0.016321655, 'queue_time': 0.220762273, 'total_time': 0.081382459}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b51da721-38d5-4e72-9b50-b36a933b6d70-0', usage_metadata={'input_tokens': 47, 'output_tokens': 31, 'total_tokens': 78})}),), interrupts=()),\n",
              " StateSnapshot(values={'messages': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f07bf19-90e0-68ad-bfff-3f36c8e9eb6b'}}, metadata={'source': 'input', 'step': -1, 'parents': {}}, created_at='2025-08-18T05:10:03.365375+00:00', parent_config=None, tasks=(PregelTask(id='d7320a64-2064-06e6-5494-da0c07789b41', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': 'Heyy, My name is Krish'}),), interrupts=())]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-L2aXnDgZ4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Long-Term Memory\n",
        " - Use long-term memory to store user-specific or application-specific data across conversations"
      ],
      "metadata": {
        "id": "zJPJog7WqKYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.store.memory import InMemoryStore\n",
        "\n",
        "#Build the Graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "def chatnode(state:State):\n",
        "  system_messages = \"You are an assitant\"\n",
        "  state[\"messages\"] = llm_model.invoke({\"system_messages\": system_messages, \"messages\": state[\"messages\"]})\n",
        "  return state\n",
        "\n",
        "#Add node\n",
        "workflow.add_node(\"chatnode\", chatnode)\n",
        "#Add edges\n",
        "workflow.add_edge(START, \"chatnode\")\n",
        "workflow.add_edge(\"chatnode\", END)\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "#Compile the graph\n",
        "graph = workflow.compile(store = store)\n"
      ],
      "metadata": {
        "id": "HaivKLJegZ88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config1 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "#First question\n",
        "response = graph.invoke({\"messages\": \"Heyy, My name is Krish and I like Vadapav\"}, config = config1)\n",
        "for message in response[\"messages\"]:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTCeE8nAgaA9",
        "outputId": "c9bc8a06-5e4c-4e1d-9c27-69ac4089a63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, My name is Krish and I like Vadapav\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Namaste Krish, nice to meet you. Vadapav is a popular Indian street food, isn't it? It's a staple in Mumbai, with a crispy vada (fried potato dumpling) served in a soft pav (bread bun) with a variety of chutneys. What's your favorite way to enjoy Vadapav - with a hot cup of chai or as a quick snack on-the-go?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config1 = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "#First question\n",
        "response = graph.invoke({\"messages\": \"Heyy, Do you not what I like?\"}, config = config1)\n",
        "for message in response[\"messages\"]:\n",
        "  message.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh8phgsDgaFh",
        "outputId": "807f2863-8396-45b6-9fa3-ad40192f0502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Heyy, Do you not what I like?\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I'm happy to chat with you. Since we just started talking, I don't have any information about your preferences or interests yet. I'm a blank slate, ready to learn and help. Would you like to share what you're into, or is there something specific you'd like to talk about? I'm all ears!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sippSXVggaJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfZzKQIwfFwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DiQUX9m6fFzM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}